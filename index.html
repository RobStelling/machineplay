<!-- Copyright 2017 Google Inc. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
-->
<!doctype html>
<meta charset="utf-8">
<head>
  <link rel="stylesheet" href="css/styles.css">
</head>
<body>
  <script src="https://distill.pub/template.v1.js"></script>
  <script type="text/front-matter">
    title: "Explaining neural network concepts through an interactive visualization"
    description: "A neural network for complementary color prediction"
    authors:
    - Roberto Stelling: http://stelling.cc/
    - Adriana Vivacqua: http://www.im.ufrj.br/visualizarDocente.php?idDepartamento=2&idDocente=147
    affiliations:
    - PPGI/UFRJ: http://www.ppgi.ufrj.br/
    - PPGI/UFRJ: http://www.ppgi.ufrj.br/
  </script>

  <dt-article>
    <h1>Explaining neural network concepts through an interactive visualization</h1>
    <h2>Using a neural network for complementary color prediction</h2>
    <dt-byline></dt-byline>
    <h2>Abstract</h2>
    <p>In this article we explore interactive data visualization as a mean to help users understand a training method of a Neural Network. The visualization let the users interact with the neural network training procedure, changing parameters and viewing results as they are shown on the browser. As most machine learning and neural network methods are opaque and hard to understand by the public in general, we explore this visualization as a way to teach the techniques and concepts around neural network model training, visually exposing the behaviour of the training algorithm to the users playing with the visualization. In this article we describe one particular exploration, using the problem of predicting the complementary color of a RGB color, implemented as a neural network model.
    </p>
    <h2>Motivation</h2>
    <p>
      <ul>
        <li>Machine Learning, Artificial Intelligence and Neural Networks techniques are being used in a broad spectrum of applications.</li>
        <li>Some techniques are hard to understand, even for advanced users <dt-cite key="lecun2015deep"></dt-cite>.</li>
        <li>This has led to a push for novel forms of interaction that make it easier for a user, at different roles, to understand how solutions are reached.</li>
        <li>Visualization techniques have been successfully used to help understand large databases and complex processes.</li>
        <li>We are investigating the possibilities of using interactive visualizations to explain machine learning techniques, providing a better understanding of how they function.</li>
        <li>In this paper, we present an interactive visualization to show how neural networks work. We selected a simple and scalable problem, calculating complementary colors, and developed an interactive visualization that allows a user to explore the effects of changing some parameters and view the training process as it happens</li>
      </ul>
    </p>
    <p>We believe that the best way to understand neural network concepts is by interacting with real-life neural network implementations. In this paper, we present an interactive web application, where it is possible to train a neural network to predict a complementary color in RGB, change its parameters, see the network converge, start over and run the model over and over again, adjusting parameters at will. Hopefully the user will eventually develop insights about neural network concepts after some time playing with the application. By the way, the parameters of the demo are a bit off to start with, so it is up to the user to play and improve them. There is an explanation of the interface <em><a href="#demo">here</a></em>, and the application is live at the <a href="#MLPlay">end of the article</a>.</p>
    <h2>Neural Networks</h2>
    <p>Neural network concepts are neither easily understood, nor easily explained. Most explanations start with the McCulloch/Pitts <dt-cite key="mcculloch1943logical"></dt-cite> neuron, build the idea of layers and weights, define backpropagation <dt-cite key="wikiBackPropagation"></dt-cite> and may, or may not, end the discussion with learning rates, batch size, epochs, activation functions and other intricacies of neural network implementations.</p>
    <p><img src="mcpitts.png"></p>
    <p>But when an student or researcher has to implement a neural network, he or she will have to manipulate the very last layer of abstraction, as almost every library deliver layers, weights and backpropagation as well wrapped black boxes. It is during the very first neural network implementation that the beginner will have a better grasp of learning rates, batch sizes, activation units, network architecture, epochs and other concepts.</p>
    <p>Keeping this student or researcher in mind, we developed a web based application, on top of <a href="https://js.tensorflow.org/">tensorflow.js</a> <dt-cite key="tensorflowjs"></dt-cite>, a javascript inception of TensorFlow<dt-cite key="tensorflow2015-whitepaper"></dt-cite> library and a spinoff of deeplearn.js (now Tensorflow.js core <dt-cite key="tensorflowjsCore"></dt-cite>). In this application the user can play with some of the model parameters (more options and functionality will appear in later versions) of an application and see the model training and predictions on a web browser, as it happens.</p>
    <h2>Visualizations and Explanations</h2>
    <p>Visualizations enable users to better understand and explore large sets of data or processes.</p>
    <p>Different types of users require different types of explanations and learning strategies. We divide users into three categories, according to how knowledgeable they are:</p>
    <p>
      <ol>
        <li>Layperson: has no knowledge of the process or algorithms being explained. This covers most of the general public;</li>
        <li>Knowledgeable: knows the concepts and algorithms, is familiar with the techniques involved;</li>
        <li>Designer: this is a person involved in the design of the solution. Although a knowledgeable individual, the designer might also need to better understand the system, in order to fine tune parameters.</li>
      </ol>
    </p>
    <p>This case was developed with knowledgeable in mind but a layperson can also explore it, as it assumes little prior knowledge of Neural Networks.</p>
    <h2>The model</h2>
    <p>To demonstrate a real neural network in action we selected the problem of predicting a complementary color <dt-cite key="wikipediaCC"></dt-cite> <dt-cite key="OxfordComplementaryColor"></dt-cite> from the RGB <dt-cite key="wikiRGBColorModel"></dt-cite> color space. The idea was adapted from a deeplearn.js (now Tensorflow.js core<dt-cite key="tensorflowjsCore"></dt-cite>) tutorial, called Complementary Color Prediction, developed by <a href="https://twitter.com/chizeng">Chi Zeng</a>, from Google. The original tutorial was developed in TypeScript <dt-cite key="typeScript"></dt-cite> but as the library was eventually deprecated, the code, apparently, is not available anymore. Initially we've built a D3.js <dt-cite key="D3js"></dt-cite> visualization for the neural network output and corrected an small issue with activation units. Later, when deeplearn.js moved to tensorflow.js, we rewrote the whole application in JavaScript with tensorflow.js and built the demo described in this article.</p>
    <h3>Why complementary colors?</h3>
    <p>The main reasons for choosing a model to predict complementary colors are:</p>
    <p>
      <ul>
        <li>The predictions can be shown visually, without much numerical information (although we also show the RGB codes of the colors).</li>
        <li>There is no direct formula to produce the complementary of a color in RGB, but there is an algorithm to generate the complementary of a RGB color. As a bonus, we show that a properly constructed neural network can learn the calculations of a complex algorithm.</li>
        <li>The problem has a good balance between complexity and scalability to fit in a browser based solution.</li>
      </ul>
    </p>
    <h3>Implementation</h3>
    <p>The model was implemented in JavaScript with an architecture of 3 hidden layers. One batch of data/label pairs is generated randomically for each training round, as shown in the code below.</p>
    <dt-code block language="javascript">
      async function train1Batch() {
        // Reduce the learning rate by 85% every 42 steps
        //currentLearningRate = initialLearningRate * Math.pow(0.85, Math.floor(step/42));
        //model.optimizer.learningRate = currentLearningRate;
        const batchData = generateData(batchSize);
        const dataTensor = color2tensor(batchData[0]);
        const labelTensor = color2tensor(batchData[1]);
        const history = await model.fit(dataTensor, labelTensor,
                 {batchSize: batchSize,
                  epochs: epochs
                 });

        cost = history.history.loss[0];
        tf.dispose(dataTensor, labelTensor);
        return step++;
      }
    </dt-code>
    <p>The labels are generated using an implementation of a complementary color algorithm based on an answer by Edd<dt-cite key="stackOverflowEdd"></dt-cite> in Stack Overflow. The full code is rather long and is available at our application repository in github <dt-cite key="RSmpGithub"></dt-cite>. The computeComplementaryColor function converts an RGB color input to HSL <dt-cite key="wikiHSLColorModel"></dt-cite>, compute the complementary color in this color space and converts the result back to RGB.</p>
    <p>At each round the model runs for 10 epochs (epoch is another parameter that can be easily exposed to the application interface, but we decided to omit it in the first version). After a user defined number of training rounds, the model is called to predict a set of test colors and the results can be visually compared, on the fly, with the reference RGB labels generated by Edd's algorithm.</p>
    <p>The output of each training round is displayed on a color wheel using D3.js <dt-cite key="D3js"></dt-cite>, as shown in the figure below.</p>
    <p><img src="MPSshot.png" width="100%" height="100%"></p>
    <h3>Neural network architecture </h3>
    <p>The neural network has an input layer for the RGB colors, 3 hidden fully connected layers with 64, 32 and 16 relu units, respectively, and the output layer, for the model predictions. The code below shows how the neural network was created.</p>
    <dt-code block language="javascript">
      function modelInit() {
        //Add input layer
        // First layer must have an input shape defined.
        model.add(tf.layers.dense({units: 3,
                                  inputShape: [3]}));
        // Afterwards, TF.js does automatic shape inference.
        model.add(tf.layers.dense({units: 64,
                                   activation: 'relu'
                                 }));
        // Afterwards, TF.js does automatic shape inference.
        model.add(tf.layers.dense({units: 32,
                                   activation: 'relu'
                                 }));
        // Afterwards, TF.js does automatic shape inference.
        model.add(tf.layers.dense({units: 16,
                                   activation: 'relu'
                                 }));
        // Afterwards, TF.js does automatic shape inference.
        model.add(tf.layers.dense({units: 3
                                 }));
      }
    </dt-code>
    <h3>Optimizer, metric and loss</h3>
    <p>We used a momentum <dt-cite key="sutskever2013importance"></dt-cite> optimizer with mean squared error loss and accuracy metric. The loss function was implemented manually, as seen on the code below, instead of using the library provided Mean Squared Error loss function. This was a design choice that allowed us to change/tweak the loss function at will during development time. On a future version we plan to provide the user with a selection of loss functions, as a configuration option.</p>
    <dt-code block language="javascript">
      function loss(predictions, labels) {
        // Subtract our labels (actual values) from predictions, square the results,
        // and take the mean. Inputs are tensors.
        return tf.tidy(() => {
          const meanSquareError = predictions.sub(labels).square().mean();
          return meanSquareError;
        });
      }
    </dt-code>

    <h2>Common Misconception</h2>
    <p>We found out that a common difficulty in understanding machine learning applications arise from a simple misconception about the very framework of a machine learning application. Most people look at the field through the lenses of a traditional application, where data is fed to an algorithm that produces an answer.</p>
    <p><img src="DataAlgorithmAnswer.png"></p>
    <p>To understand machine learning applications, one must first understand how they come to life: a LARGE amount of data is fed to a training algorithm and it produces, as output, an instance of a prediction algorithm trained with the input data that was provided to the machine learning algorithm. This output can then take data as input and produce predictions in accordance with the training process. There is NO guarantees that the predictions will be fully correct but they will, hopefully, be as good as possible.</p>
    <p><img src="DataMLAlgorithm.png"></p>
    <h2>An analogy for the layperson</h2>
    <p>Suppose that you are given a card with a color at random and you must give back a card with a color too, it can be the same color or a different one. In the beginning, as you have no training, the best you can do is to give back a card at random. Being a deterministic algorithm, everytime someone gives you a card with a color X, you will produce as a response, the same Y color. So if I give you red and you give me back a green card, next time I give you a red card you will give me back a green card again. At this juncture you will produce bogus answers, but answers you will produce!</p>
    <p>Now suppose that you are given set of n card pairs, with the correct input/output cards. You can then use these pairs to correct your card selections. But there is a catch, whenever you correct the predictions for one card (for example, blue is the correct answer to red), all the other predictions are affected in a way, they change a little bit. What you try to do then, is to minimize the error of the color pairs you got as input.</p>
    <p>The idea is that, after this training and adjusting procedure, you may be able to make good guesses on color pairs you saw, and, hopefully, you will be able to make good guesses on the colors you never saw!</p>
    <p>Now, you can adjust the color predictions in small baby steps, changing the output colors just a little bit (remember that when you change the answer for one color, all the other answers are slightly affected), or you can adjust the colors in big steps, with a big effect on your other predictions. Or you can do something in between these two extremes.</p>
    <p>Every time you get a color pair list, you adjust the color predictions again. When will you stop this training process? Maybe you will stop after training for 1.000 rounds, or when your data has finished, or you will stop when the error in all the colors you saw is smaller than a certain value (meaning that youâ€™re doing good predictions on this data). It is up to you.</p>
    <p>If you are still following us, then you, likely, understood what were you doing with the color cards. We can now throw a bit of nomenclature:</p>
    <p>
      <ul>
        <li><b>Learning Rate</b>: Is the amount of change you make on each pass. Big changes correspond to high learning rate, small changes correspond to small learning rate. If the learning rate is too big, you may never be able to make good predictions. If the learning rate is too small, it may take very long to properly correct, and later predict, the color pairs.</li>
        <li><b>Batch Size</b>: Is the amount of color pairs you get at each turn. The bigger the batch size, the better your adjustments, but you will take longer to process them. Notice, though, that you can process the colors in parallel, so a small batch size can be a bad thing.</li>
        <li><b>Step Limit</b>: Is the maximum amount of rounds of training that you will go through.</li>
        <li><b>Cost Target</b>: The the minimum error that your overall card predictions must reach in order to stop the training.</li>
      </ul>
    </p>
    <p>There are another concepts in neural network training, but the four above are the ones you will see at work in the interactive application.</p>


    <h2 idu="demo">The demo</h2>
    <p>The application we present is based on one of the tutorials for the late deeplearn.js library (now <a href="https://js.tensorflow.org/">tensorflow.js</a>, a javascript inception of TensorFlow<dt-cite key="tensorflow2015-whitepaper"></dt-cite> and a spinoff of deeplearn.js). That tutorial, called Complementary Color Prediction, was developed by <a href="https://twitter.com/chizeng">Chi Zeng</a>, from Google.</p>
    <p>We developed a D3.js <dt-cite key="D3js"></dt-cite> visual interface for the CCP tutorial in typescript and later, when deeplearn.js moved to tensorflow.js, we rewrote the whole code in javascript and added a bit of interactivity on top of it.</p>
    <p><img src="MPStart.png" width="100%" height="100%"></p>
    <p>In the figure above we show the starting stage of the demo. The inner ring of the display is a set of colors that we will use to predict its complementary colors <dt-cite key="OxfordComplementaryColor"></dt-cite><dt-cite key="wikipediaCC"></dt-cite>. In the middle ring we show the corresponding complementary colors, for reference and control. In the outer ring we display the model predicted complementary colors, for each color in the original color set (the inner ring). In the beginning the outer ring is gray, as no color was predicted yet. When running, the outer ring will be updated with the model predicted colors, and the user will be able to compare the predictions with the reference complementary colors in the middle ring.</p>
    <p>It is possible to play with the demo using the controls below:</p>
    <div  id="MLPlayDemo1">
      <table>
        <tr>
          <th id="startStopDemo1" class="demo" width="5%">Start</th>
          <th class="demo">Reset</th>
          <th class="demo">Learning Rate</th>
          <th class="demo">Batch Size</th>
          <th class="demo">Render interval</th>
          <th class="demo">Step Limit</th>
          <th class="demo">Cost Target</th>
        </tr>
        <tr>
          <td>
            <div class="tooltip">
              <span class="tooltiptext">Starts/stops the network training</span>
              <label class="switch demo">
                <input id="triggerDemo1" type="checkbox">
                <span class="slider demo round trigger"></span>
              </label>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <spah class="tooltiptext">Resets the network, looses all training but keeps parameter values</spah>
              <label class="switch demo">
                <input id="updateDemo1" type="checkbox" checked disabled>
                <span class="slider demo round"></span>
              </label>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Learning Rate controls how much the weights of the network are adjusted</span>
                <input type="range" min="0.001" max="0.759" value="0.42" step="0.001" class="rangeslider demo freeze Demo1" id="learning_rangeDemo1">
                <span id="learning_valDemo1" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Batch size is the amount of data sent to the network per training round</span>
                <input type="range" min="1" max="128" value="10" class="rangeslider demo freeze Demo1" id="batch_rangeDemo1">
                <span id="batch_valDemo1" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Number of network training runs before displaying the learned colors</span>
                <input type="range" min="1" max="50" value="5" class="rangeslider demo" id="render_rangeDemo1">
                <span id="render_valDemo1" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Number of learning rounds before stopping</span>
                <input type="range" min="25" max="10000" value="1000" class="rangeslider demo" id="step_rangeDemo1">
                <span id="step_valDemo1" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Stops learning if cost is smaller than Cost Target</span>
                <input type="range" min="0.00001" max="0.001" value="0.0005" step="0.00001" class="rangeslider demo" id="cost_rangeDemo1">
                <span id="cost_valDemo1" class="demoDisplay"></span>
              </div>
            </div>
          </td>
        </tr>
      </table>
    </div>
    <p>The five sliders on the right side let the user adjust the values for:</p>
    <ul>
      <li><b>Learning Rate</b>: The learning rate of the training model. Smaller learning rates are the baby steps that may go in the right direction but will take longer to get to the right prediction, larger learning rates are big strides that may go to the right direction but will likely miss the prediction altogether, by going beyond the expected solution.</li>
      <li><b>Batch Size</b>: The amount of colors that will be used for training at each round, ranging from 1 to 128. There are drawbacks in small batches and big batches, it is up to the analyst to figure out the <em>best batch</em>.</li>
      <li><b>Render Interval</b>: The number of rounds between screen updates, ranging from 1 (updating every round) to 50.</li>
      <li><b>Step Limit</b>: The number of rounds that the training will run, ranging from 25 to 10.000. If the training reaches this limit it will stop and this slider color will switch to red, to show that a step limit was reached. If the step limit is too small, the predictions won't be that good (the outer ring colors won't match the middle ring colors).</li>
      <li><b>Cost Target</b>: The cost target of the predictions, ranging from 0.00005 to 0.001. The training will stop when the cost of the predictions is smaller than the cost target, and the cost target slider will become red in consequence, as shown in the figure below. If the cost is too big, the training may not converge, if the cost is too high, the training may overfit (becoming a very good predictor of the colors it has seen, but off the mark on the colors it hasn't seen).</li>
    </ul>
    <div  id="MLPlayDemo2">
      <table>
        <tr>
          <th id="startStopDemo2" class="demo" width="5%">Start</th>
          <th class="demo">Reset</th>
          <th class="demo">Learning Rate</th>
          <th class="demo">Batch Size</th>
          <th class="demo">Render interval</th>
          <th class="demo">Step Limit</th>
          <th class="demo">Cost Target</th>
        </tr>
        <tr>
          <td>
            <div class="tooltip">
              <span class="tooltiptext">Starts/stops the network training</span>
              <label class="switch demo">
                <input id="triggerDemo2" type="checkbox">
                <span class="slider demo round trigger"></span>
              </label>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <spah class="tooltiptext">Resets the network, looses all training but keeps parameter values</spah>
              <label class="switch demo">
                <input id="updateDemo2" type="checkbox" checked>
                <span class="slider demo round"></span>
              </label>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Learning Rate controls how much the weights of the network are adjusted</span>
                <input type="range" min="0.001" max="0.759" value="0.42" step="0.001" class="rangeslider demo freeze Demo2" id="learning_rangeDemo2" disabled>
                <span id="learning_valDemo2" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Batch size is the amount of data sent to the network per training round</span>
                <input type="range" min="1" max="128" value="10" class="rangeslider demo freeze Demo2" id="batch_rangeDemo2" disabled>
                <span id="batch_valDemo2" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Number of network training runs before displaying the learned colors</span>
                <input type="range" min="1" max="50" value="5" class="rangeslider demo" id="render_rangeDemo2">
                <span id="render_valDemo2" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Number of learning rounds before stopping</span>
                <input type="range" min="25" max="10000" value="1000" class="rangeslider demo" id="step_rangeDemo2">
                <span id="step_valDemo2" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Stops learning if cost is smaller than Cost Target</span>
                <input type="range" min="0.00001" max="0.001" value="0.0005" step="0.00001" class="rangeslider demo finish Demo2" id="cost_rangeDemo2">
                <span id="cost_valDemo2" class="demoDisplay"></span>
              </div>
            </div>
          </td>
        </tr>
      </table>
    </div>
    <p>In the beginning the user can adjust any of the five sliders, choosing whatever combination he/she like. The user has to click on the start button to start the training. The colors on the outer ring will change, as the training goes on. Each new update shows the predictions of the neural network algorithm for the complementary colors at that stage of the training. The use can compare the results with the colors in the middle ring. When the training is running, the Start button becomes a red Stop button, as shown in the figure below. The user can click on the stop button to halt the model training.</p>
    <div  id="MLPlayDemo3">
      <table>
        <tr>
          <th id="startStopDemo3" class="demo" width="5%">Stop</th>
          <th class="demo">Reset</th>
          <th class="demo">Learning Rate</th>
          <th class="demo">Batch Size</th>
          <th class="demo">Render interval</th>
          <th class="demo">Step Limit</th>
          <th class="demo">Cost Target</th>
        </tr>
        <tr>
          <td>
            <div class="tooltip">
              <span class="tooltiptext">Starts/stops the network training</span>
              <label class="switch demo">
                <input id="triggerDemo3" type="checkbox" checked>
                <span class="slider demo round trigger"></span>
              </label>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <spah class="tooltiptext">Resets the network, looses all training but keeps parameter values</spah>
              <label class="switch demo">
                <input id="updateDemo3" type="checkbox" checked disabled>
                <span class="slider demo round"></span>
              </label>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Learning Rate controls how much the weights of the network are adjusted</span>
                <input type="range" min="0.001" max="0.759" value="0.42" step="0.001" class="rangeslider demo freeze Demo3" id="learning_rangeDemo3" disabled>
                <span id="learning_valDemo3" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Batch size is the amount of data sent to the network per training round</span>
                <input type="range" min="1" max="128" value="10" class="rangeslider demo freeze Demo3" id="batch_rangeDemo3" disabled>
                <span id="batch_valDemo3" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Number of network training runs before displaying the learned colors</span>
                <input type="range" min="1" max="50" value="5" class="rangeslider demo" id="render_rangeDemo3">
                <span id="render_valDemo3" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Number of learning rounds before stopping</span>
                <input type="range" min="25" max="10000" value="1000" class="rangeslider demo" id="step_rangeDemo3">
                <span id="step_valDemo3" class="demoDisplay"></span>
              </div>
            </div>
          </td>
          <td>
            <div class="tooltip">
              <div class="""slidecontainer">
                <span class="tooltiptext">Stops learning if cost is smaller than Cost Target</span>
                <input type="range" min="0.00001" max="0.001" value="0.0005" step="0.00001" class="rangeslider demo" id="cost_rangeDemo3">
                <span id="cost_valDemo3" class="demoDisplay"></span>
              </div>
            </div>
          </td>
        </tr>
      </table>
    </div>
    <p>After the training starts, the user won't be able to change neither the Learning Rate nor the Batch Size, so these parameter must be wisely choosen. The training may stop for three reasons:</p>
    <ol>
      <li>The user clicked on <em>Stop</em>.</li>
      <li>The <em>Step Limit</em> was reached.</li>
      <li>The <em>Cost Target</em> was reached.</li>
    </ol>
    <p>If the training was manually halted, then the user can restart it by clicking on start again. If the <em>Step Limit</em> was reached, the user can <b>increase</b> the step limit, if possible, and restart the training. If the <em>Cost Target</em> was reached, the user can <b>decrease</b> the cost target, if possible, and restart the training. Whenever the training is halted, the user can Reset it by clicking on the <em>Reset</em> button (notice that the button is disabled when the training is running). A reset will clear all the training data (meaning that the model will "forget" the predictions made so far), but will keep the values chosen for the training session. This way the user can incrementally fine tune the training and explore the impact of each parameter change.</p>
    <p>Notice that the initial parameters are not optimal, so the user must play with them a little bit before finding out a good workable combination of parameter input. The user is encouraged to try different combinations, play with them and, with luck,  acquire some insights on how these parameters influence the training and the model predictions. One important thing to notice about machine learning training is that there is a random effect going on. The user will see it in action by running the model with the same parameters more then once. He/She won't get the same results twice! The demo the be found at the <a href="#MLPlay">end of the article.</a></p>
  </dt-article>

  <dt-appendix>
    <h3>Apendix</h3>
    <h4>Acknowledgements</h4>
    <p>First and foremost we would like to thank the TensorFlowJS team, this is an amazing library that opens new doors to machine learning explorations on the browser. Tensorflow.js code base can be found <a href="https://github.com/tensorflow/tfjs">here</a>. A special thanks goes to <a href="https://twitter.com/chizeng">Chi Zeng</a>, whose code inspired this application.</p> 
  </dt-appendix>

  <script type="text/bibliography">
    @article{gregor2015draw,
      title={DRAW: A recurrent neural network for image generation},
      author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
      journal={arXivreprint arXiv:1502.04623},
      year={2015},
      url={https://arxiv.org/pdf/1502.04623.pdf}
    }
    @book{shelley2009frankenstein,
      title={Frankenstein, or, The Modern Prometheus, 1818},
      author={Shelley, Mary Wollstonecraft},
      year={2009},
      publisher={Engage Books, AD Classic}
    }
    @article{mcculloch1943logical,
      title={A logical calculus of the ideas immanent in nervous activity},
      author={McCulloch, Warren S and Pitts, Walter},
      journal={The bulletin of mathematical biophysics},
      volume={5},
      number={4},
      pages={115--133},
      year={1943},
      publisher={Springer}
    }
    @book{simak1977skirmish,
      title={Skirmish: The Great Short Fiction of Clifford D. Simak},
      author={Simak, Clifford D},
      year={1977},
      publisher={Putnam}
    }
    @inproceedings{sutskever2013importance,
      title={On the importance of initialization and momentum in deep learning},
      author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
      booktitle={International conference on machine learning},
      pages={1139--1147},
      year={2013}
    }
    @misc{wikipediaCC,
      title={Complementary Colors},
      author={Wikipedia},
      year={2018},
      howpublished={\url{https://en.wikipedia.org/wiki/Complementary_colors}},
      url={https://en.wikipedia.org/wiki/Complementary_colors}
    }
    @misc{wikiBackPropagation,
      title={Backpropagation},
      author={Wikipedia},
      year={2018},
      howpublished={\url{https://en.wikipedia.org/wiki/Backpropagation}},
      url={https://en.wikipedia.org/wiki/Backpropagation}
    }
    @misc{wikiRGBColorModel,
      title={RGB Color Model},
      author={Wikipedia},
      year={2018},
      howpublished={\url{https://en.wikipedia.org/wiki/RGB_color_model}},
      url={https://en.wikipedia.org/wiki/RGB_color_model}
    }
    @misc{wikiHSLColorModel,
      title={HSL and HSV Color Model},
      author={Wikipedia},
      year={2018},
      howpublished={\url{https://en.wikipedia.org/wiki/HSL_and_HSV}},
      url={https://en.wikipedia.org/wiki/HSL_and_HSV}
    }
    @misc{OxfordComplementaryColor,
      title={Oxford Learner's Dictionary - Complementary Color},
      author={Oxford University Press},
      year={2018},
      howpublished={\url{https://www.oxfordlearnersdictionaries.com/definition/american_english/complementary-color}},
      url={https://www.oxfordlearnersdictionaries.com/definition/american_english/complementary-color}
    }
    @misc{tensorflowjsCore,
      title={Tensorflow.js core},
      author={Tensorflow.js Team},
      year={2018},
      howpublished={\url{https://github.com/tensorflow/tfjs-core}},
      url={https://github.com/tensorflow/tfjs-core}
    }
    @misc{tensorflowjs,
      title={Tensorflow.js},
      author={Tensorflow.js Team},
      year={2018},
      howpublished={\url{https://js.tensorflow.org/}},
      url={https://js.tensorflow.org/}
    }
    @misc{RSmpGithub,
      title={Explaining neural network concepts through an interactive visualization},
      author={Stelling, Roberto},
      year={2018},
      howpublished={\url{https://github.com/RobStelling/machineplay}},
      url={https://github.com/RobStelling/machineplay}
    }
    @article{lecun2015deep,
      title={Deep learning},
      author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
      journal={nature},
      volume={521},
      number={7553},
      pages={436},
      year={2015},
      publisher={Nature Publishing Group}
    }
    @misc{typeScript,
      title={TypeScript - JavaScript that scales},
      author={{Microsoft}},
      year={2018},
      howpublished={\url{https://www.typescriptlang.org/}},
      url={https://www.typescriptlang.org/}
    }
    @misc{D3js,
      title={D3.js},
      author={Bostok, Mike},
      year={2018},
      howpublished={\url{https://d3js.org/}},
      url={https://d3js.org/}
    }
    @misc{stackOverflowEdd,
      title={Complementary Color in RGB - StackOverflow},
      author={Edd},
      year={2016},
      howpublished={\url{https://stackoverflow.com/a/37657940}},
      url={https://stackoverflow.com/a/37657940}
    }
    @misc{tensorflow2015-whitepaper,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    note={Software available from tensorflow.org},
    author={
        Mart\'{\i}n~Abadi and
        Ashish~Agarwal and
        Paul~Barham and
        Eugene~Brevdo and
        Zhifeng~Chen and
        Craig~Citro and
        Greg~S.~Corrado and
        Andy~Davis and
        Jeffrey~Dean and
        Matthieu~Devin and
        Sanjay~Ghemawat and
        Ian~Goodfellow and
        Andrew~Harp and
        Geoffrey~Irving and
        Michael~Isard and
        Yangqing Jia and
        Rafal~Jozefowicz and
        Lukasz~Kaiser and
        Manjunath~Kudlur and
        Josh~Levenberg and
        Dandelion~Man\'{e} and
        Rajat~Monga and
        Sherry~Moore and
        Derek~Murray and
        Chris~Olah and
        Mike~Schuster and
        Jonathon~Shlens and
        Benoit~Steiner and
        Ilya~Sutskever and
        Kunal~Talwar and
        Paul~Tucker and
        Vincent~Vanhoucke and
        Vijay~Vasudevan and
        Fernanda~Vi\'{e}gas and
        Oriol~Vinyals and
        Pete~Warden and
        Martin~Wattenberg and
        Martin~Wicke and
        Yuan~Yu and
        Xiaoqiang~Zheng},
      year={2015},
    }
  </script>
  <center>
  <div style="overflow-x:auto;" id="MLPlay">
    <table>
      <tr>
        <th id="startStop">Start</th>
        <th>Reset</th>
        <th>Learning Rate</th>
        <th>Batch Size</th>
        <th>Render interval</th>
        <th>Step Limit</th>
        <th>Cost Target</th>
      </tr>
      <tr>
        <td>
          <div class="tooltip">
            <span class="tooltiptext">Starts/stops the network training</span>
            <label class="switch">
              <input id="trigger" type="checkbox">
              <span class="slider round trigger"></span>
            </label>
          </div>
        </td>
        <td>
          <div class="tooltip">
            <spah class="tooltiptext">Resets the network, looses all training but keeps parameter values</spah>
            <label class="switch">
              <input id="update" type="checkbox" checked disabled>
              <span class="slider round"></span>
            </label>
          </div>
        </td>
        <td>
          <div class="tooltip">
            <div class="""slidecontainer">
              <span class="tooltiptext">Learning Rate controls how much the weights of the network are adjusted</span>
              <input type="range" min="0.001" max="0.759" value="0.001" step="0.001" class="rangeslider freeze" id="learning_range">
              <span id="learning_val"></span>
            </div>
          </div>
        </td>
        <td>
          <div class="tooltip">
            <div class="""slidecontainer">
              <span class="tooltiptext">Batch size is the amount of data sent to the network per training round</span>
              <input type="range" min="1" max="128" value="1" class="rangeslider freeze" id="batch_range">
              <span id="batch_val"></span>
            </div>
          </div>
        </td>
        <td>
          <div class="tooltip">
            <div class="""slidecontainer">
              <span class="tooltiptext">Number of network training runs before displaying the learned colors</span>
              <input type="range" min="1" max="50" value="1" class="rangeslider" id="render_range">
              <span id="render_val"></span>
            </div>
          </div>
        </td>
        <td>
          <div class="tooltip">
            <div class="""slidecontainer">
              <span class="tooltiptext">Number of learning rounds before stopping</span>
              <input type="range" min="25" max="10000" value="25" class="rangeslider" id="step_range">
              <span id="step_val"></span>
            </div>
          </div>
        </td>
        <td>
          <div class="tooltip">
            <div class="""slidecontainer">
              <span class="tooltiptext">Stops learning if cost is smaller than Cost Target</span>
              <input type="range" min="0.00001" max="0.001" value="0.00001" step="0.00001" class="rangeslider" id="cost_range">
              <span id="cost_val"></span>
            </div>
          </div>
        </td>
      </tr>
    </table>
  </div>
</center>
<center>

  <svg id="DLCCP" viewBox="-600 -600 1200 1200" width="100%" height="100%"></svg>
</center>
  <table id='color-table' border='0' style='display:none'>
    <tr>
      <th>Original Color</th>
      <th>Actual Complement</th>
      <th>Predicted Complement</th>
    </tr>
    <tr data-original-color='244,67,54'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='233,30,99'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='156,39,176'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='103,58,183'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='63,81,181'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='33,150,243'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='76,175,80'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='139,195,74'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='205,220,57'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='255,235,59'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='255,193,7'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='255,152,0'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='255,87,34'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='121,85,72'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='158,158,158'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='96,125,139'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='0,0,0'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  </table>
  <script src="https://d3js.org/d3.v4.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/d3-annotation/2.1.0/d3-annotation.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.12.0"></script>
  <script src='tf_ccp.js'></script>
  <script src='demoControls.js'></script>
  <script>
    controls("Demo1");
    controls("Demo2");
    controls("Demo3");
  </script>
</body>
