<!-- Copyright 2017 Google Inc. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================
-->

<!doctype html>



<!doctype html>
<meta charset="utf-8">
<head>
  <link rel="stylesheet" href="css/styles.css">
</head>
<body>
  <script src="https://distill.pub/template.v1.js"></script>
  <script type="text/front-matter">
    title: "Explaining neural network concepts through an interactive visualization"
    description: "A neural network for complementary color prediction"
    authors:
    - Roberto Stelling: http://stelling.cc/
    - Adriana Vivacqua: http://www.im.ufrj.br/visualizarDocente.php?idDepartamento=2&idDocente=147
    affiliations:
    - PPGI/UFRJ: http://www.ppgi.ufrj.br/
    - PPGI/UFRJ: http://www.ppgi.ufrj.br/
  </script>

  <dt-article>
    <h1>Explaining neural network concepts through an interactive visualization</h1>
    <h2>Using a neural network for complementary color prediction</h2>
    <dt-byline></dt-byline>
    <h2>Abstract</h2>
    <p>We explore visualization as a mean to help an user understand a training method of a Neural Network. By allowing him/her to interact with the neural network training procedure, changing parameters and viewing results on the fly. Most machine learning and neural network methods are opaque and hard to understand by the public in general. We explore this visualization as a way to teach the techniques and concepts around neural network model training, visually exposing the behaviour of the training algorithm. In this article we describe one particular exploration, using the problem of computing the complementary color of an RGB color, using a neural network to predict a complementary color given color/labels pairs as training input. An interactive visualization was built to help the user understand the training model parameters and the convergence of a neural network, through the visualization of the model results at each step.
    </p>
    <h2>Motivation</h2>
    <p>
      <ul>
        <li>Machine Learning, Artificial Intelligence and Neural Networks techniques are being used in a broad spectrum of applications.</li>
        <li>Some techniques are hard to understand, even for advance users <dt-cite key="lecun2015deep"></dt-cite>.</li>
        <li>This has led to a push for novel forms of interaction that make it easier for a user, at different roles, to understand how solutions are reached.</li>
        <li>Visualization techniques have been successfully used to help understand large databases and complex processes.</li>
        <li>We are investigating the possibilities of using visualization to explain machine learning techniques, providing a better understanding of how they function.</li>
        <li>In this paper, we present an interactive visualization to show how neural networks work. We selected a simple and scalable problem, calculating complementary colors, and developed an interactive visualization that allows a user to explore the effects of changing some parameters and view the training process as it happens</li>
      </ul>
    </p>
    <p>We believe that the best way to understand neural network concepts is by interacting with real-life neural network implementations. In this paper, we present an interactive web application, where it is possible to train a neural network to predict a complementary color in RGB, change its parameters, see the network converge, start over and run the model over and over again, adjusting parameters at will. Hopefully the user will eventually develop insights about neural network concepts after some time playing with the application. By the way, the parameters of the demo are a bit off to start with, so it is up to the user to play and improve them. There is an explanation of the interface <em><a href="#demo">here</a></em>, and the application is live at the <a href="#MLPlay">end of the article</a>.</p>
    <h2>Neural Networks</h2>
    <p>Neural network concepts are neither easily understood, nor easily explained. Most explanations start with the McCulloch/Pitts <dt-cite key="mcculloch1943logical"></dt-cite> neuron, build the idea of layers and weights, define backpropagation <dt-cite key="wikiBackPropagation"></dt-cite> and may, or may not, end the discussion with learning rates, batch size, epochs, activation functions and other intricacies of neural network implementations.</p>
    <p><img src="mcpitts.png"></p>
    <p>But when an student or researcher has to implement a neural network, he or she will have to manipulate the very last layer of abstraction, as almost every library deliver layers, weights and backpropagation as well wrapped black boxes. It is during the very first neural network implementation that the beginner will have a better grasp of learning rates, batch sizes, activation units, network architecture, epochs and other concepts.</p>
    <p>Keeping this student or researcher in mind, we developed a web based application, on top of <a href="https://js.tensorflow.org/">tensorflow.js</a> <dt-cite key="tensorflowjs"></dt-cite>, a javascript inception of TensorFlow<dt-cite key="tensorflow2015-whitepaper"></dt-cite> library and a spinoff of deeplearn.js (now Tensorflow.js core <dt-cite key="tensorflowjsCore"></dt-cite>). In this application the user can play with some of the model parameters (more options and functionality will appear in later versions) of an application and see the model training and predictions on a web browser, as it happens.</p>
    <h2>Visualizations and Explanations</h2>
    <p>Visualizations enable users to better understand and explore large sets of data or processes.</p>
    <p>Different types of users require different types of explanations and learning strategies. We divide users into three categories, according to how knowledgeable they are:</p>
    <p>
      <ol>
        <li>Layperson: has no knowledge of the process or algorithms being explained. This covers most of the general public;</li>
        <li>Knowledgeable: knows the concepts and algorithms, is familiar with the techniques involved;</li>
        <li>Designer: this is a person involved in the design of the solution. Although a knowledgeable individual, the designer might also need to better understand the system, in order to fine tune parameters.</li>
      </ol>
    </p>
    <p>This case was developed with knowledgeable in mind but a layperson can also explore it, as it assumes little prior knowledge of Neural Networks.</p>
    <h2>The model</h2>
    <p>To demonstrate a real neural network in action we selected the problem of predicting a complementary color <dt-cite key="wikipediaCC"></dt-cite> <dt-cite key="OxfordComplementaryColor"></dt-cite> from the RGB <dt-cite key="wikiRGBColorModel"></dt-cite> color space. The idea was adapted from a deeplearn.js (now Tensorflow.js core<dt-cite key="tensorflowjsCore"></dt-cite>) tutorial, called Complementary Color Prediction, developed by <a href="https://twitter.com/chizeng">Chi Zeng</a>, from Google. The original tutorial was developed in TypeScript but as the library was eventually deprecated, the code is not available anymore. Initially we've built a D3.js <dt-cite key="D3js"></dt-cite> visualization for the neural network output and corrected an small issue with activation units. Later, when deeplearn.js moved to tensorflow.js, we rewrote the whole application in JavaScript with tensorflow.js and built the demo described in this article.</p>
    <h3>Why complementary colors?</h3>
    <p>The main reasons for choosing a model to predict complementary colors are:</p>
    <p>
      <ul>
        <li>The predictions can be shown visually, without much numerical information (although we also show the RGB codes of the colors).</li>
        <li>There is no direct formula to produce the complementary of a color in RGB, but there is an algorithm to generate the complementary of a RGB color. As a bonus, we show that a properly constructed neural network can learn the calculations of a complex algorithm.</li>
        <li>The problem has a good balance between complexity and scalability to fit in a browser based solution.</li>
      </ul>
    </p>
    <h3>Implementation</h3>
    <p>The model was implemented in JavaScript with an architecture of 3 hidden layers. One batch of data/label pairs is generated randomically for each training round, as shown in the code below.</p>
    <dt-code block language="javascript">
      async function train1Batch() {
        // Reduce the learning rate by 85% every 42 steps
        //currentLearningRate = initialLearningRate * Math.pow(0.85, Math.floor(step/42));
        //model.optimizer.learningRate = currentLearningRate;
        const batchData = generateData(batchSize);
        const dataTensor = color2tensor(batchData[0]);
        const labelTensor = color2tensor(batchData[1]);
        const history = await model.fit(dataTensor, labelTensor,
                 {batchSize: batchSize,
                  epochs: epochs
                 });

        cost = history.history.loss[0];
        tf.dispose(dataTensor, labelTensor);
        return step++;
      }
    </dt-code>
    <p>The labels are generated using an implementation of a complementary color algorithm based on an answer by Edd<dt-cite key="stackOverflowEdd"></dt-cite> in Stack Overflow. The full code is rather long and is available at our application repository in github <dt-cite key="RSmpGithub"></dt-cite>. The computeComplementaryColor function converts an RGB color input to HSL <dt-cite key="wikiHSLColorModel"></dt-cite>, compute the complementary color in this color space and converts the result back to RGB.</p>
    <p>At each round the model runs for 10 epochs (epoch is another parameter that can be easily exposed to the application interface). After a user defined number of training rounds, the model is called to predict a set of test colors and the results can be visually compared, on the fly, with the reference RGB labels generated by the previous algorithm.</p>
    <p>The output of each training round is displayed on a color wheel using D3.js <dt-cite key="D3js"></dt-cite>, as shown in the figure below.</p>
    <p><img src="MPSshot.png" width="100%" height="100%"></p>
    <h3>Neural network architecture </h3>
    <p>The neural network has an input layer for the RGB colors, 3 hidden fully connected layers with 64, 32 and 16 relu units, and an output layer, for the model prediction. The code below shows how the neural network was created.</p>
    <dt-code block language="javascript">
      function modelInit() {
        //Add input layer
        // First layer must have an input shape defined.
        model.add(tf.layers.dense({units: 3,
                                  activation: 'tanh',
                                  inputShape: [3]}));
        // Afterwards, TF.js does automatic shape inference.
        model.add(tf.layers.dense({units: 64,
                                   activation: 'relu'
                                 }));
        // Afterwards, TF.js does automatic shape inference.
        model.add(tf.layers.dense({units: 32,
                                   activation: 'relu'
                                 }));
        // Afterwards, TF.js does automatic shape inference.
        model.add(tf.layers.dense({units: 16,
                                   activation: 'relu'
                                 }));
        // Afterwards, TF.js does automatic shape inference.
        model.add(tf.layers.dense({units: 3,
                                   activation: 'tanh'
                                 }));
      }
    </dt-code>
    <h3>Optimizer, metric and loss</h3>
    <p>We used a momentum <dt-cite key="sutskever2013importance"></dt-cite> optimizer with mean squared error loss and accuracy metric. The loss function was implemented manually, as seen on the code below, instead of using the library provided Mean Squared Error loss function. This was a design choice that allowed us to change/tweak the loss function at will during development time. On a future version we plan to provide the user with a selection of loss functions, as a configuration option.</p>
    <dt-code block language="javascript">
      function loss(predictions, labels) {
        // Subtract our labels (actual values) from predictions, square the results,
        // and take the mean. Inputs are tensors.
        return tf.tidy(() => {
          const meanSquareError = predictions.sub(labels).square().mean();
          return meanSquareError;
        });
      }
    </dt-code>

    <h2>Common Misconception</h2>
    <p>We found out that a common difficulty in understanding machine learning applications arise from a simple misconception about the very framework of a machine learning application. Most people look at the field through the lenses of a traditional application, where data is fed to an algorithm that produces an answer.</p>
    <p><img src="DataAlgorithmAnswer.png"></p>
    <p>To understand machine learning applications, one must first understand how they come to life: a LARGE amount of data is fed to a training algorithm and it produces, as output, an instance of a prediction algorithm trained with the input data that was provided to the machine learning algorithm. This output can then take other data as input and produce predictions in accordance with the training process. There is NO guarantees that the predictions will be fully correct but they will, likely, be as good as possible.</p>
    <p><img src="DataMLAlgorithm.png"></p>
    <h2>An analogy for the layperson</h2>
    <p>Suppose that you are given a card with a color at random and you must give back a card with a color too, it can be the same color or a different one.In the beginning, you have no training, the best you can do is to give back a card at random. Being a deterministic algorithm, everytime someone gives you a card with a color X, you will produce as a response, the same Y color. So if I give you red and you give me back a green card, next time I give you a red card you will give me back a green card again. At this juncture you will produce bogus answers, but answers you will produce!</p>
    <p>Now suppose that you are given set of n card pairs, with the correct input/output cards. You can then use these pairs to correct your card selections. But there is a catch, whenever you correct the predictions for one card (for example, blue is the correct answer to red), all the other predictions are affected in a way, they change a little bit. What you try to do then, is to minimize the error of the color pairs you got as input.</p>
    <p>The idea is that, after this training and adjusting procedure, you may be able to make a good guess on color pairs you saw, and, hopefully, you will be able to make a good guess on the colors you never saw!</p>
    <p>Now, you can adjust the color predictions in small baby steps, changing the output colors just a little bit (remember that when you change the answer for one color, all the other answers are slightly affected), while trying to improve the colors you know the answers to, or you can adjust the colors in big steps, changing your other predictions a lot. Or you can do something in between these two extremes.</p>
    <p>Every time you get a color pair list, you adjust the color predictions again. When will you stop this training process? Maybe you will stop after training for 1.000 rounds, or when your data has finished, or you will stop when the error in all the colors you saw is smaller than a certain value (meaning that you’re doing good predictions on this data). It is up to you.</p>
    <p>If you are still following us, then you, hopefully, understood what were you doing with the color cards. We can now throw a bit of nomenclature:</p>
    <p>
      <ul>
        <li><b>Learning Rate</b>: Is the amount of change you make on each pass. Big changes correspond to high learning rate, small changes correspond to small learning rate. If the learning rate is too big, you may never be able to make good predictions. If the learning rate is too small, it may take very long to properly correct, and later predict, the color pairs.</li>
        <li><b>Batch Size</b>: Is the amount of color pairs you get at each turn. The bigger the batch size, the better your adjustments, but you will take longer to process them. Notice, though, that you can process the colors in parallel, so a small batch size can be a bad thing.</li>
        <li><b>Step Limit</b>: Is the maximum amount of rounds of training that you will go through.</li>
        <li><b>Cost Target</b>: The the minimum error that your overall card predictions must reach in order to stop the training.</li>
      </ul>
    </p>
    <p>There are another concepts in neural network training, but the for above are the ones you will see at work in the interactive application.</p>


    <h2 id="demo">The demo</h2>
    <p>The application you are going to see was based in one of the tutorials for the late deeplearn.js library (now <a href="https://js.tensorflow.org/">tensorflow.js, a javascript inception of TensorFlow<dt-cite key="tensorflow2015-whitepaper"></dt-cite></a>). That tutorial, called Complementary Color Prediction, was developed by <a href="https://twitter.com/chizeng">Chi Zeng</a>, from Google.</p>
    <p>We created a D3 <dt-cite key="D3js"></dt-cite> visual interface for the CCP tutorial in typescript and later, when deeplearn.js moved to tensorflow.js, we rewrote the whole code and added a bit of interactivity to it.</p>
    <p><img src="MPStart.png" width="100%" height="100%"></p>
    <p>In the figure above you can see the staring point of the demo. The inner ring of the display is a set of colors that we will use to predict its complementary colors <dt-cite key="OxfordComplementaryColor"></dt-cite><dt-cite key="wikipediaCC"></dt-cite>. In the middle ring you will find the computed complementary colors, for reference. In the outer ring you will find the model predicted complementary colors. In the beginning the outer ring is gray, as no color was predicted yet. When running, the outer ring will be updated with the model predicted colors, and you can compare the predictions with the computed colors in the middle ring.</p>
    <p>You can play with the demo using the controls below:</p>
    <p><img src="MPControls.png" width="100%" height="100%"></p>
    <p>The five sliders on the right let you adjust the values for:</p>
    <ul>
      <li><b>Learning Rate</b>: The learning rate of your training model. Smaller learning rates are the baby steps that may go in the right direction but will take longer to get to the right prediction, larger learning rates are big strides that may go to the right direction but miss the prediction altogether, by going beyond the expected solution.</li>
      <li><b>Batch Size</b>: The amount of colors that will be used for training at each round, ranging from 1 to 128. There are drawbacks in small batches and big batches, it is up to the analyst to figure out the <em>best batch</em> size (pun intended).</li>
      <li><b>Render Interval</b>: The number of rounds between screen updates, ranging from 1 (updating every round) to 50.</li>
      <li><b>Step Limit</b>: The number of rounds that the training will run, ranging from 25 to 10.0000. If the training reaches this limit it will stop and this slider will be in red, to show that a step limit was reached. If the step limit is too small, the predictions won't be that good (the outer ring colors won't match the middle ring colors).</li>
      <li><b>Cost Target</b>: The cost target of the predictions, ranging from 0.00005 to 0.001. The training will stop when the cost of the predictions is smaller than the cost target, and the cost target slider will become red, as in the figure below. If the cost is too big, the training may not converge, if the cost is too high, the training may overfit (turning into a very good predictor withthe colors it has seen, but off the mark on the colors it hasn't seen).</li>
    </ul>
    <p><img src="MPControlsCostTarget.png" width="100%" height="100%"></p>
    <p>In the beginning you can adjust any of the five slides, choosing whatever combination you like. To start to play, click on the start button and watch the colors on the outer ring change, as the training goes on. Each new update shows the predictions of the algorithm to the complementary colors at that stage of the training, and you can compare them with the middle ring. When the training is running, the Start button becomes a red Stop button, as shown in the figure below.</p>
    <p><img src="MPControlsRunning.png" width="100%" height="100%"></p>
    <p>After the train starts, you won't be able to change neither the Learning Rate nor the Batch Size, so choose them wisely. The training may stop for three reasons:</p>
    <ol>
      <li>You clicked on <em>Stop</em>.</li>
      <li>The <em>Step Limit</em> was reached.</li>
      <li>The <em>Cost Target</em> was reached.</li>
    </ol>
    <p>If you stopped the training, you can restart it by clicking on start again. If the <em>Step Limit</em> was reached, you can <b>increase</b> the limit and restart the training. If the <em>Cost Target</em> was reached, you can <b>decrease</b> the target and start again. Whenever the training is halted, you can reset it by clicking on the <em>Reset</em> button (notice that the button is disabled when the training is on). A reset will clear all the training (meaning that the model will "forget" the predictions), but will keep the values you chose for the training, for an initial training round. This way you can fine tune the training and explore the impact of each parameter change.</p>
    <p>Notice that the initial parameters are not optimal, so you must play with them a little bit before finding a good balance. Try different combinations, play with it and, with luck, you will have some insights on how these parameters influence the training and the model predictions. One important thing to notice about machine learning training is that there is a random effect going on. You will see it in action by running the model with the same parameters more then once, you will not get the same results twice! So what are you doing here ? <a href="#MLPlay">Let's play</a>!</p>
  </dt-article>

  <dt-appendix>
    <h3>Apendix</h3>
    <h4>Acknowledgements</h4>
    <p>First and foremost we would like to thank the TensorFlowJS team, this is an amazing library that opens new doors to machine learning explorations on the browser. The code base can be found <a href="https://github.com/tensorflow/tfjs">here</a>. A special thanks goes to <a href="https://twitter.com/chizeng">Chi Zeng</a>, whose code inspired this application.</p> 
  </dt-appendix>

  <script type="text/bibliography">
    @article{gregor2015draw,
      title={DRAW: A recurrent neural network for image generation},
      author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
      journal={arXivreprint arXiv:1502.04623},
      year={2015},
      url={https://arxiv.org/pdf/1502.04623.pdf}
    }
    @book{shelley2009frankenstein,
      title={Frankenstein, or, The Modern Prometheus, 1818},
      author={Shelley, Mary Wollstonecraft},
      year={2009},
      publisher={Engage Books, AD Classic}
    }
    @article{mcculloch1943logical,
      title={A logical calculus of the ideas immanent in nervous activity},
      author={McCulloch, Warren S and Pitts, Walter},
      journal={The bulletin of mathematical biophysics},
      volume={5},
      number={4},
      pages={115--133},
      year={1943},
      publisher={Springer}
    }
    @book{simak1977skirmish,
      title={Skirmish: The Great Short Fiction of Clifford D. Simak},
      author={Simak, Clifford D},
      year={1977},
      publisher={Putnam}
    }
    @inproceedings{sutskever2013importance,
      title={On the importance of initialization and momentum in deep learning},
      author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
      booktitle={International conference on machine learning},
      pages={1139--1147},
      year={2013}
    }
    @misc{wikipediaCC,
      title={Complementary Colors},
      author={Wikipedia},
      year={2018},
      howpublished={\url{https://en.wikipedia.org/wiki/Complementary_colors}},
      url={https://en.wikipedia.org/wiki/Complementary_colors}
    }
    @misc{wikiBackPropagation,
      title={Backpropagation},
      author={Wikipedia},
      year={2018},
      howpublished={\url{https://en.wikipedia.org/wiki/Backpropagation}},
      url={https://en.wikipedia.org/wiki/Backpropagation}
    }
    @misc{wikiRGBColorModel,
      title={RGB Color Model},
      author={Wikipedia},
      year={2018},
      howpublished={\url{https://en.wikipedia.org/wiki/RGB_color_model}},
      url={https://en.wikipedia.org/wiki/RGB_color_model}
    }
    @misc{wikiHSLColorModel,
      title={HSL and HSV Color Model},
      author={Wikipedia},
      year={2018},
      howpublished={\url{https://en.wikipedia.org/wiki/HSL_and_HSV}},
      url={https://en.wikipedia.org/wiki/HSL_and_HSV}
    }
    @misc{OxfordComplementaryColor,
      title={Oxford Learner's Dictionary - Complementary Color},
      author={Oxford University Press},
      year={2018},
      howpublished={\url{https://www.oxfordlearnersdictionaries.com/definition/american_english/complementary-color}},
      url={https://www.oxfordlearnersdictionaries.com/definition/american_english/complementary-color}
    }
    @misc{tensorflowjsCore,
      title={Tensorflow.js core},
      author={Tensorflow.js Team},
      year={2018},
      howpublished={\url{https://github.com/tensorflow/tfjs-core}},
      url={https://github.com/tensorflow/tfjs-core}
    }
    @misc{tensorflowjs,
      title={Tensorflow.js},
      author={Tensorflow.js Team},
      year={2018},
      howpublished={\url{https://js.tensorflow.org/}},
      url={https://js.tensorflow.org/}
    }
    @misc{RSmpGithub,
      title={Explaining neural network concepts through an interactive visualization},
      author={Stelling, Roberto},
      year={2018},
      howpublished={\url{https://github.com/RobStelling/machineplay}},
      url={https://github.com/RobStelling/machineplay}
    }
    @article{lecun2015deep,
      title={Deep learning},
      author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
      journal={nature},
      volume={521},
      number={7553},
      pages={436},
      year={2015},
      publisher={Nature Publishing Group}
    }
    @misc{typeScript,
      title={TypeScript - JavaScript that scales},
      author={{Microsoft Corporation}},
      year={2018},
      howpublished={\url{https://www.typescriptlang.org/}},
      url={https://www.typescriptlang.org/}
    }
    @misc{D3js,
      title={D3.js},
      author={Bostok, Mike},
      year={2018},
      howpublished={\url{https://d3js.org/}},
      url={https://d3js.org/}
    }
    @misc{stackOverflowEdd,
      title={Complementary Color in RGB - StackOverflow},
      author={Edd},
      year={2016},
      howpublished={\url{https://stackoverflow.com/a/37657940}},
      url={https://stackoverflow.com/a/37657940}
    }
    @misc{tensorflow2015-whitepaper,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    note={Software available from tensorflow.org},
    author={
        Mart\'{\i}n~Abadi and
        Ashish~Agarwal and
        Paul~Barham and
        Eugene~Brevdo and
        Zhifeng~Chen and
        Craig~Citro and
        Greg~S.~Corrado and
        Andy~Davis and
        Jeffrey~Dean and
        Matthieu~Devin and
        Sanjay~Ghemawat and
        Ian~Goodfellow and
        Andrew~Harp and
        Geoffrey~Irving and
        Michael~Isard and
        Yangqing Jia and
        Rafal~Jozefowicz and
        Lukasz~Kaiser and
        Manjunath~Kudlur and
        Josh~Levenberg and
        Dandelion~Man\'{e} and
        Rajat~Monga and
        Sherry~Moore and
        Derek~Murray and
        Chris~Olah and
        Mike~Schuster and
        Jonathon~Shlens and
        Benoit~Steiner and
        Ilya~Sutskever and
        Kunal~Talwar and
        Paul~Tucker and
        Vincent~Vanhoucke and
        Vijay~Vasudevan and
        Fernanda~Vi\'{e}gas and
        Oriol~Vinyals and
        Pete~Warden and
        Martin~Wattenberg and
        Martin~Wicke and
        Yuan~Yu and
        Xiaoqiang~Zheng},
      year={2015},
    }
  </script>
  <center>
  <div style="overflow-x:auto;" id="MLPlay">
    <table>
      <tr>
        <th id="startStop">Start</th>
        <th>Reset</th>
        <th>Learning Rate</th>
        <th>Batch Size</th>
        <th>Render interval</th>
        <th>Step Limit</th>
        <th>Cost Target</th>
      </tr>
      <tr>
        <td>
          <label class="switch">
            <input id="trigger" type="checkbox">
            <span class="slider round trigger"></span>
          </label>
        </td>
        <td>
          <label class="switch">
            <input id="update" type="checkbox" checked disabled>
            <span class="slider round"></span>
          </label>
        </td>
        <td>
          <div class="""slidecontainer">
            <input type="range" min="0.001" max="0.759" value="0.001" step="0.001" class="rangeslider freeze" id="learning_range">
            <span id="learning_val"></span>
          </div>
        </td>
        <td>
          <div class="""slidecontainer">
            <input type="range" min="1" max="128" value="1" class="rangeslider freeze" id="batch_range">
            <span id="batch_val"></span>
          </div>
        </td>
        <td>
          <div class="""slidecontainer">
            <input type="range" min="1" max="50" value="1" class="rangeslider" id="render_range">
            <span id="render_val"></span>
          </div>
        </td>
        <td>
          <div class="""slidecontainer">
            <input type="range" min="25" max="10000" value="25" class="rangeslider" id="step_range">
            <span id="step_val"></span>
          </div>
        </td>
        <td>
          <div class="""slidecontainer">
            <input type="range" min="0.00005" max="0.001" value="0.00005" step="0.00001" class="rangeslider" id="cost_range">
            <span id="cost_val"></span>
          </div>
        </td>
      </tr>
    </table>
  </div>
</center>
<center>

  <svg id="DLCCP" viewBox="-600 -600 1200 1200" width="100%" height="100%"></svg>
</center>
  <table id='color-table' border='0' style='display:none'>
    <tr>
      <th>Original Color</th>
      <th>Actual Complement</th>
      <th>Predicted Complement</th>
    </tr>
    <tr data-original-color='244,67,54'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='233,30,99'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='156,39,176'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='103,58,183'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='63,81,181'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='33,150,243'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='76,175,80'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='139,195,74'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='205,220,57'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='255,235,59'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='255,193,7'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='255,152,0'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='255,87,34'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='121,85,72'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='158,158,158'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='96,125,139'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr data-original-color='0,0,0'>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  </table>
  <script src="https://d3js.org/d3.v4.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/d3-annotation/2.1.0/d3-annotation.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.12.0"></script>
  <script src='tf_ccp.js'></script>
</body>
